{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 0. Data Exploration\nInspect the raw `.npz` file structure before loading the full dataset."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "import numpy as np\nimport zipfile\nimport io\nfrom pathlib import Path\nimport matplotlib.pyplot as plt\n\n# \u2500\u2500 Point this at ONE of your Town zip files \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nCHANNEL_ZIP = Path(\"/Volumes/multi_modal/sunny/Channel Data/V2I/Nt_1_16_Nr_1_16_fc_28GHz/Town03.zip\")\n\nprint(\"=\" * 70)\nprint(\"CHANNEL ZIP CONTENTS\")\nprint(\"=\" * 70)\n\nwith zipfile.ZipFile(CHANNEL_ZIP, \"r\") as z:\n    all_files = z.namelist()\n    npz_files = [f for f in all_files if f.endswith(\"_paths.npz\")]\n    yaml_files = [f for f in all_files if f.endswith(\".yaml\")]\n    print(f\"  .npz files  : {len(npz_files)}\")\n    print(f\"  .yaml files : {len(yaml_files)}\")\n    print(f\"  First 5 npz : {npz_files[:5]}\")\n\n    # Load the first .npz for inspection\n    raw = z.read(npz_files[0])\n\nnpz = np.load(io.BytesIO(raw))\nprint(\"\\n\" + \"=\" * 70)\nprint(f\"NPZ FILE: {npz_files[0]}\")\nprint(\"=\" * 70)\nprint(f\"  Keys: {npz.files}\")\nfor k in npz.files:\n    arr = npz[k]\n    print(f\"  [{k}]  shape={arr.shape}  dtype={arr.dtype}  \"\n          f\"min={np.abs(arr).min():.4g}  max={np.abs(arr).max():.4g}\")\n",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# \u2500\u2500 Inspect channel tensor 'a' in detail \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\na = npz[\"a\"]\nprint(\"Raw 'a' shape:\", a.shape)\n\n# Squeeze singleton dims \u2192 (Nr, Nt, n_paths)\na_sq = np.squeeze(a)\nif a_sq.ndim == 2:\n    a_sq = a_sq[:, :, np.newaxis]\nnr, nt, n_paths = a_sq.shape\nprint(f\"Squeezed: Nr={nr}, Nt={nt}, n_paths={n_paths}\")\n\n# Path-level power (sum over antennas)\npath_power = np.sum(np.abs(a_sq) ** 2, axis=(0, 1))  # (n_paths,)\ntotal_power = path_power.sum()\nprint(f\"\\nPath powers : {path_power.round(6)}\")\nprint(f\"Total power : {total_power:.6g}\")\nprint(f\"Dominant path index : {np.argmax(path_power)}\")\nprint(f\"Dominant path ratio : {path_power.max() / (total_power + 1e-12):.3f}\")\n\n# Compute beam index (Tx antenna with highest received power)\ntx_power = np.sum(np.abs(a_sq) ** 2, axis=(0, 2))   # (Nt,)\nbeam_index = int(np.argmax(tx_power))\nprint(f\"\\nTx antenna powers (beam codebook): {tx_power.round(4)}\")\nprint(f\"Beam index (argmax Tx power)      : {beam_index}\")\n",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# \u2500\u2500 Sample-level label distribution across N frames \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nN_SAMPLE = 200   # how many .npz files to scan for quick stats\n\nbeam_list = []\nwith zipfile.ZipFile(CHANNEL_ZIP, \"r\") as z:\n    npz_files_scan = [f for f in z.namelist() if f.endswith(\"_paths.npz\")][:N_SAMPLE]\n    for fname in npz_files_scan:\n        raw = z.read(fname)\n        npz_s = np.load(io.BytesIO(raw))\n        a_s = np.squeeze(npz_s[\"a\"])\n        if a_s.ndim == 2:\n            a_s = a_s[:, :, np.newaxis]\n        tx_p = np.sum(np.abs(a_s) ** 2, axis=(0, 2))\n        beam_list.append(int(np.argmax(tx_p)))\n\nbeam_arr = np.array(beam_list)\ncounts = np.bincount(beam_arr, minlength=16)\nprint(f\"Beam distribution over {len(beam_arr)} frames:\")\nfor i, c in enumerate(counts):\n    bar = \"\u2588\" * int(c / max(counts) * 30)\n    print(f\"  Beam {i:2d} | {c:4d} | {bar}\")\n\nfig, ax = plt.subplots(figsize=(9, 3))\nax.bar(range(16), counts, color=\"steelblue\", edgecolor=\"white\")\nax.set_xlabel(\"Beam Index\")\nax.set_ylabel(\"Count\")\nax.set_title(f\"Beam Index Distribution (first {len(beam_arr)} frames, Town03)\")\nax.set_xticks(range(16))\nplt.tight_layout()\nplt.show()\n",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 1. Imports & Reproducibility"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "import numpy as np\nimport random\nimport os\nimport re\nimport io\nimport zipfile\nimport pickle\nimport hashlib\nimport time\nfrom dataclasses import dataclass\nfrom datetime import datetime\nfrom pathlib import Path, PurePosixPath\nfrom typing import Dict, List, Optional, Sequence, Tuple, Union\n\nimport flwr as fl\nimport ray\nimport tensorflow as tf\nfrom tensorflow import keras\nimport pandas as pd\n\n# \u2500\u2500 Reproducibility \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nseed = 42\nrandom.seed(seed)\nnp.random.seed(seed)\ntf.random.set_seed(seed)\nos.environ[\"TF_DETERMINISTIC_OPS\"] = \"1\"\nos.environ[\"PYTHONHASHSEED\"] = str(seed)\ntf.keras.backend.clear_session()\n\nresults_dir = \"experiment_results\"\ntimestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\nprint(\"Random seeds set. TF version:\", tf.__version__)\n",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 2. Configuration"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "CFG = {\n    # Training\n    \"local_epochs\"   : 3,\n    \"lr\"             : 1e-3,\n    \"test_size\"      : 0.2,\n\n    # Beam focal loss\n    \"focal_gamma\"    : 2.0,   # 0 = standard cross-entropy\n    \"label_smoothing\": 0.05,\n    \"grad_clip_norm\" : 5.0,\n\n    # Federated\n    \"client_frac\"    : 1.0,\n\n    # Dataset\n    \"outage_threshold_db\": {\"outage_lt\": 0.0, \"degraded_lt\": 10.0},\n    \"noise_power_ref\"    : 1e-11,\n}\n",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 3. Dataset"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "def _parse_total_antennas(config_name: str, side: str) -> int:\n    m = re.search(rf\"{side}_(\\d+)_(\\d+)\", config_name)\n    if not m:\n        raise ValueError(f\"Could not parse {side} from: {config_name}\")\n    return int(m.group(1)) * int(m.group(2))\n\n\ndef _safe_isfinite(x: np.ndarray) -> bool:\n    if np.iscomplexobj(x):\n        return np.isfinite(x.real).all() and np.isfinite(x.imag).all()\n    return np.isfinite(x).all()\n\n\n@dataclass(frozen=True)\nclass ChannelSampleRef:\n    zip_path: Path\n    inner_npz: str\n\ndef generate_dft_codebook(size: int, num_beams: int) -> np.ndarray:\n    \"\"\"\n    DFT steering-vector codebook  (MMW paper Eq. 2 / Eq. 4).\n\n    f(q)[n] = (1 / sqrt(Nt)) * exp(j * 2*pi / Q * n * q)\n\n    Parameters\n    ----------\n    size      : Nt  \u2013 number of Tx antennas\n    num_beams : Q   \u2013 number of DFT beams (paper uses Q=64)\n\n    Returns\n    -------\n    codebook : complex64 array of shape (size, num_beams)\n               Column q is the steering vector for beam q.\n    \"\"\"\n    n = np.arange(size).reshape(-1, 1)          # (Nt, 1)\n    q = np.arange(num_beams).reshape(1, -1)     # (1,  Q)\n    # Complex exponent:  e^{j * 2\u03c0/Q * n * q}\n    codebook = (1.0 / np.sqrt(size)) * np.exp(\n        1j * (2 * np.pi / num_beams) * n * q\n    )\n    return codebook.astype(np.complex64)        # (Nt, Q)\n",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "class ChannelDataset:\n    \"\"\"\n    Channel-only dataset over *_paths.npz files inside TownXX.zip.\n    Each sample = ONE frame_id = ONE *_paths.npz.\n    Only 'beam_index' label is produced (beam-only mode).\n\n    Feature tensor shape: (Nr, Nt, K, 3)\n      channel 0 : real part of a_sq\n      channel 1 : imag part of a_sq\n      channel 2 : magnitude |a_sq|  (professor's stable-training feature)\n\n    Beam index uses DFT codebook (MMW paper Eq. 2 / Eq. 4):\n      H = sum_paths(a_sq)           shape (Nr, Nt)\n      beam_responses = H @ codebook  shape (Nr, Q)\n      beam_index = argmax over q of ||beam_responses[:,q]||^2\n    \"\"\"\n\n    NUM_DFT_BEAMS: int = 64   # Q in the paper (fixed even when Nt=16)\n\n    def __init__(\n        self,\n        root: Union[str, Path],\n        config_name: str,\n        towns: Optional[Sequence[str]] = None,\n        scenario_contains: Optional[str] = None,\n        cav_contains: Optional[str] = None,\n        stride: int = 1,\n        expected_num_paths: Optional[int] = None,\n        assert_no_nans: bool = True,\n        assert_shapes: bool = True,\n        top_k_paths: Optional[int] = None,\n        pad_if_short: bool = True,\n    ):\n        self.root = Path(root)\n        self.config_name = config_name\n        self.config_dir = self.root / \"sunny\" / \"Channel Data\" / \"V2I\" / config_name\n\n        if not self.config_dir.exists():\n            raise FileNotFoundError(f\"Config dir not found: {self.config_dir}\")\n\n        self.nt = _parse_total_antennas(config_name, \"Nt\")\n        self.nr = _parse_total_antennas(config_name, \"Nr\")\n\n        self.towns = list(towns) if towns else None\n        self.scenario_contains = scenario_contains\n        self.cav_contains = cav_contains\n        self.stride = max(1, int(stride))\n        self.expected_num_paths = expected_num_paths\n        self.assert_no_nans = assert_no_nans\n        self.assert_shapes = assert_shapes\n        self.top_k_paths = top_k_paths\n        self.pad_if_short = pad_if_short\n\n        # \u2500\u2500 CHANGE 2: DFT codebook  (Nt x Q) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n        self.codebook: np.ndarray = generate_dft_codebook(\n            size=self.nt, num_beams=self.NUM_DFT_BEAMS\n        )  # complex64, shape (Nt, Q)\n\n        self.index: List[ChannelSampleRef] = self._build_index()\n        if self.stride > 1:\n            self.index = self.index[:: self.stride]\n\n        self._expected_csi_shape: Optional[Tuple[int, ...]] = None\n\n    # \u2500\u2500 Index building \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n    def _build_index(self) -> List[ChannelSampleRef]:\n        if self.towns is None:\n            zips = sorted(self.config_dir.glob(\"Town*.zip\"))\n        else:\n            zips = [self.config_dir / f\"{t}.zip\" for t in self.towns]\n\n        refs: List[ChannelSampleRef] = []\n        for zp in zips:\n            if not zp.exists():\n                continue\n            with zipfile.ZipFile(zp, \"r\") as z:\n                for name in z.namelist():\n                    if not name.endswith(\"_paths.npz\"):\n                        continue\n                    p = PurePosixPath(name)\n                    if self.scenario_contains and self.scenario_contains.lower() not in str(p).lower():\n                        continue\n                    if self.cav_contains and self.cav_contains.lower() not in str(p).lower():\n                        continue\n                    refs.append(ChannelSampleRef(zip_path=zp, inner_npz=name))\n\n        def sort_key(ref: ChannelSampleRef):\n            p = PurePosixPath(ref.inner_npz)\n            m = re.match(r\"(\\d+)_paths$\", p.stem)\n            return (str(ref.zip_path), str(p.parent), int(m.group(1)) if m else -1)\n\n        refs.sort(key=sort_key)\n        if not refs:\n            raise ValueError(f\"No *_paths.npz found under {self.config_dir}\")\n        return refs\n\n    # \u2500\u2500 Metadata \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n    def _parse_metadata(self, inner_path: str) -> Dict[str, str]:\n        p = PurePosixPath(inner_path)\n        m = re.match(r\"(\\d+)_paths\\.npz$\", p.name)\n        frame_id = int(m.group(1)) if m else -1\n        cav_id = p.parent.name if \"cav\" in p.parent.name.lower() else \"unknown\"\n        location = p.parent.parent.name or \"unknown\"\n        return {\"location\": location, \"cav_id\": cav_id, \"frame_id\": frame_id}\n\n    def get_sample_metadata(self, idx: int) -> Dict[str, str]:\n        ref = self.index[idx]\n        meta = self._parse_metadata(ref.inner_npz)\n        meta[\"town\"] = ref.zip_path.stem\n        meta[\"zip_path\"] = str(ref.zip_path)\n        meta[\"inner_path\"] = ref.inner_npz\n        return meta\n\n    def build_metadata_index(self) -> pd.DataFrame:\n        rows = []\n        for idx in range(len(self)):\n            meta = self.get_sample_metadata(idx)\n            meta[\"sample_idx\"] = idx\n            rows.append(meta)\n        return pd.DataFrame(rows)\n\n    # \u2500\u2500 Core loading \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n    def __len__(self) -> int:\n        return len(self.index)\n\n    def _load_npz(self, ref: ChannelSampleRef) -> Dict[str, np.ndarray]:\n        with zipfile.ZipFile(ref.zip_path, \"r\") as z:\n            raw = z.read(ref.inner_npz)\n        npz = np.load(io.BytesIO(raw))\n        return {k: npz[k] for k in npz.files}\n\n    def _extract_csi(self, arrays: Dict[str, np.ndarray]) -> np.ndarray:\n        \"\"\"\n        Returns float32 tensor of shape (Nr, Nt, K, 3):\n          ch0 : real(a_sq)\n          ch1 : imag(a_sq)\n          ch2 : |a_sq|      \u2190 magnitude (professor's stable feature, CHANGE 4)\n\n        Complex arithmetic is kept throughout; abs() is taken last\n        so no information is lost before the feature extraction.\n        \"\"\"\n        a = arrays[\"a\"]  # raw complex, e.g. (1,1,Nr,1,Nt,n_paths,1)\n        if self.assert_no_nans:\n            assert _safe_isfinite(a), \"Non-finite values in 'a'\"\n\n        # Squeeze all singleton dims \u2192 (Nr, Nt, n_paths)  [CHANGE 5: keep complex]\n        a_sq = np.squeeze(a).astype(np.complex64)\n        if a_sq.ndim == 2:\n            a_sq = a_sq[:, :, np.newaxis]\n        if a_sq.ndim != 3:\n            raise ValueError(f\"Unexpected squeezed 'a' shape: {a_sq.shape}\")\n\n        # \u2500\u2500 Top-K path selection (by power, descending) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n        if self.top_k_paths is not None:\n            K = self.top_k_paths\n            # power per path: sum |a|^2 over Nr and Nt dimensions\n            path_power = np.sum(np.abs(a_sq) ** 2, axis=(0, 1))  # (n_paths,)\n            topk_idx = np.argsort(path_power)[::-1][:K]           # descending\n            a_sq = a_sq[:, :, topk_idx]                           # (Nr, Nt, K)\n            if a_sq.shape[2] < K and self.pad_if_short:\n                pad = K - a_sq.shape[2]\n                # Pad with zeros (complex zero keeps real/imag/mag all zero)\n                a_sq = np.pad(a_sq, ((0, 0), (0, 0), (0, pad)), mode=\"constant\")\n\n        # \u2500\u2500 Build 3-channel feature tensor  (CHANGE 4) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n        # Keep all complex math above this line; take abs only here.\n        real_ch = a_sq.real.astype(np.float32)          # (Nr, Nt, K)\n        imag_ch = a_sq.imag.astype(np.float32)          # (Nr, Nt, K)\n        mag_ch  = np.abs(a_sq).astype(np.float32)       # (Nr, Nt, K)  |a|\n\n        csi = np.stack([real_ch, imag_ch, mag_ch], axis=-1)  # (Nr, Nt, K, 3)\n\n        if self.assert_no_nans:\n            assert _safe_isfinite(csi), \"Non-finite values in csi tensor\"\n        return csi\n\n    def compute_beam_index(self, csi_complex: np.ndarray) -> int:\n        \"\"\"\n        DFT codebook beam selection  (MMW paper Eq. 2 / Eq. 4).  (CHANGE 3)\n\n        Parameters\n        ----------\n        csi_complex : complex64 array, shape (Nr, Nt, n_paths)\n\n        Steps\n        -----\n        1. Sum paths \u2192 H of shape (Nr, Nt)     [effective channel matrix]\n        2. H @ codebook  \u2192 shape (Nr, Q)       [steer toward each beam]\n        3. power[q] = ||col q||^2 = sum_r |response[r, q]|^2\n        4. return argmax(power)\n        \"\"\"\n        # CHANGE 5: ensure we are working with complex data\n        H = np.sum(csi_complex.astype(np.complex64), axis=2)  # (Nr, Nt)\n\n        # Matrix multiply: (Nr, Nt) @ (Nt, Q) \u2192 (Nr, Q)\n        beam_responses = H @ self.codebook   # complex64, (Nr, Q)\n\n        # Power per beam: sum |response|^2 over receive antennas\n        # np.abs handles complex correctly \u2192 real-valued magnitude\n        power = np.sum(np.abs(beam_responses) ** 2, axis=0)   # (Q,)\n\n        return int(np.argmax(power))\n\n    def __getitem__(self, idx: int) -> Tuple[np.ndarray, Dict[str, np.ndarray]]:\n        ref = self.index[idx]\n        arrays = self._load_npz(ref)\n\n        # Keep complex for beam-index computation\n        a_sq = np.squeeze(arrays[\"a\"]).astype(np.complex64)\n        if a_sq.ndim == 2:\n            a_sq = a_sq[:, :, np.newaxis]\n\n        csi_tensor = self._extract_csi(arrays)  # (Nr, Nt, K, 3)  float32\n\n        if self.assert_shapes:\n            if self._expected_csi_shape is None:\n                self._expected_csi_shape = tuple(csi_tensor.shape)\n            else:\n                assert tuple(csi_tensor.shape) == self._expected_csi_shape, (\n                    f\"CSI shape mismatch: {self._expected_csi_shape} vs {csi_tensor.shape}\"\n                )\n\n        labels = {\n            \"beam_index\": np.array(self.compute_beam_index(a_sq), dtype=np.int64)\n        }\n        return csi_tensor, labels\n",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# \u2500\u2500 DFT Codebook Sanity Check \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n# Run before training to verify shapes and beam-index logic.\nimport numpy as np\n\n# 1. Codebook shape\ncb = generate_dft_codebook(size=16, num_beams=64)\nassert cb.shape == (16, 64), f\"Bad codebook shape: {cb.shape}\"\nassert cb.dtype == np.complex64\n\n# 2. Columns are unit-norm (||f(q)||_2 = 1)\nnorms = np.linalg.norm(cb, axis=0)\nassert np.allclose(norms, 1.0, atol=1e-5), f\"Codebook columns not unit-norm: {norms[:4]}\"\n\n# 3. Orthogonality: F^H F should be ~identity  (DFT property)\ngram = cb.conj().T @ cb           # (64, 64)\noff_diag_max = np.abs(gram - np.eye(64)).max()\nassert off_diag_max < 1e-4, f\"Codebook not unitary, max off-diag: {off_diag_max:.2e}\"\n\nprint(\"\u2713 Codebook shape   :\", cb.shape)\nprint(\"\u2713 Column norms     : all ~1.0\")\nprint(f\"\u2713 Orthogonality    : max off-diagonal = {off_diag_max:.2e}\")\n\n# 4. Simulate a channel snapshot and verify output shapes\nrng = np.random.default_rng(0)\nnr, nt, n_paths = 16, 16, 6\na_fake = (rng.standard_normal((nr, nt, n_paths))\n        + 1j * rng.standard_normal((nr, nt, n_paths))).astype(np.complex64)\n\n# beam index\nH = np.sum(a_fake, axis=2)                   # (Nr, Nt)\nbeam_responses = H @ cb                       # (Nr, 64)\npower = np.sum(np.abs(beam_responses) ** 2, axis=0)  # (64,)\nbeam_idx = int(np.argmax(power))\nassert 0 <= beam_idx < 64\nprint(f\"\u2713 Beam index range : {beam_idx}  (valid 0\u201363)\")\n\n# csi tensor channels\nreal_ch = a_fake.real.astype(np.float32)\nimag_ch = a_fake.imag.astype(np.float32)\nmag_ch  = np.abs(a_fake).astype(np.float32)\ncsi = np.stack([real_ch, imag_ch, mag_ch], axis=-1)  # (16,16,6,3)\nassert csi.shape == (nr, nt, n_paths, 3), f\"Bad csi shape: {csi.shape}\"\nassert csi.dtype == np.float32\nprint(f\"\u2713 CSI tensor shape : {csi.shape}  [real, imag, |a|]\")\nprint(\"\\nAll checks passed.\")\n",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 4. Trajectory-Based Client Builder"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "class DatasetSplitter:\n    \"\"\"Trajectory-aware train/test splitting (1 client = 1 CAV trajectory).\"\"\"\n\n    def __init__(self, dataset: ChannelDataset):\n        self.dataset = dataset\n        self.metadata_df = dataset.build_metadata_index()\n        print(f\"Metadata index built: {len(self.metadata_df)} samples\")\n\n    def get_trajectory_groups(self) -> Dict[str, List[int]]:\n        trajectories = {}\n        for (town, location, cav_id), group in self.metadata_df.groupby(\n            [\"town\", \"location\", \"cav_id\"]\n        ):\n            traj_id = f\"{town}_{location}_{cav_id}\"\n            trajectories[traj_id] = group.sort_values(\"frame_id\")[\"sample_idx\"].tolist()\n        return trajectories\n\n    def split_trajectory_temporal(\n        self, indices: List[int], train_ratio: float = 0.7\n    ) -> Tuple[List[int], List[int]]:\n        split = int(len(indices) * train_ratio)\n        return indices[:split], indices[split:]\n\n\n@dataclass\nclass ChannelClientData:\n    train_indices: List[int]\n    test_indices: List[int]\n    client_id: int\n    trajectory_id: str\n\n\ndef build_clients(\n    dataset: ChannelDataset,\n    train_ratio: float = 0.7,\n    min_trajectory_length: int = 10,\n) -> List[ChannelClientData]:\n    splitter = DatasetSplitter(dataset)\n    trajectories = splitter.get_trajectory_groups()\n    print(f\"Total trajectories: {len(trajectories)}\")\n\n    trajectories = {\n        t: idx for t, idx in trajectories.items() if len(idx) >= min_trajectory_length\n    }\n    print(f\"After length filter (>={min_trajectory_length}): {len(trajectories)}\")\n\n    clients = []\n    for cid, traj_id in enumerate(sorted(trajectories)):\n        train_idx, test_idx = splitter.split_trajectory_temporal(\n            trajectories[traj_id], train_ratio\n        )\n        if not train_idx or not test_idx:\n            continue\n        clients.append(\n            ChannelClientData(train_idx, test_idx, cid, traj_id)\n        )\n    print(f\"Clients created: {len(clients)}\")\n    return clients\n",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 5. Load Dataset & Build Clients"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "ds = ChannelDataset(\n    root=\"/Volumes/multi_modal\",\n    config_name=\"Nt_1_16_Nr_1_16_fc_28GHz\",\n    towns=[\"Town03\", \"Town05\"],\n    stride=10,\n    top_k_paths=6,\n    pad_if_short=True,\n    expected_num_paths=None,\n)\nprint(f\"Dataset size: {len(ds)} samples\")\n\nx, y = ds[0]\nprint(f\"CSI shape : {x.shape}\")   # (Nr, Nt, K, 2)\nprint(f\"Labels    : {y}\")          # {'beam_index': ...}\n",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "clients = build_clients(ds, train_ratio=0.7, min_trajectory_length=10)\n",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 6. Compute Beam Class Weights"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "print(\"Collecting beam labels...\")\nall_beam = np.array([ds[i][1][\"beam_index\"] for i in range(len(ds))])\nprint(f\"Collected {len(all_beam)} samples\")\n\nbeam_counts = np.bincount(all_beam, minlength=16)\nbeam_weights = len(all_beam) / (16 * beam_counts + 1e-9)\nbeam_weights = beam_weights / beam_weights.sum() * 16\n\nprint(\"\\nBeam class weights:\")\nfor i, w in enumerate(beam_weights):\n    print(f\"  Beam {i:2d}: {w:.4f}\")\n\nnp.save(\"beam_class_weights.npy\", beam_weights)\nprint(\"\\nSaved \u2192 beam_class_weights.npy\")\n",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 7. Model Architecture (Beam Only)"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "class ChannelEncoder(tf.keras.Model):\n    \"\"\"Convolutional path encoder with attention pooling.\"\"\"\n\n    def __init__(self, nr: int, nt: int, top_k_paths: int,\n                 beam_codebook_size: int, emb_dim: int = 128, dropout: float = 0.0):\n        super().__init__()\n        self.nr = nr\n        self.nt = nt\n        self.k = top_k_paths\n        self.emb_dim = emb_dim\n\n        self.conv1 = keras.layers.Conv2D(32, 3, padding=\"same\", activation=\"relu\")\n        self.conv2 = keras.layers.Conv2D(64, 3, padding=\"same\", activation=\"relu\")\n        self.conv3 = keras.layers.Conv2D(64, 1, padding=\"same\", activation=\"relu\")\n        self.gap = keras.layers.GlobalAveragePooling2D()\n        self.att_dense = keras.layers.Dense(1)\n        self.proj = keras.Sequential([\n            keras.layers.Dense(256, activation=\"relu\"),\n            keras.layers.Dropout(dropout),\n            keras.layers.Dense(emb_dim),\n        ])\n\n    def call(self, x, training=False):\n        # x: (B, Nr, Nt, P, 3) or (Nr, Nt, P, 3)  [real, imag, mag]\n        if tf.rank(x) == 4:\n            x = tf.expand_dims(x, 0)\n\n        # Select top-K paths by power\n        power = tf.reduce_sum(tf.square(x), axis=[0, 1, 2, 4])\n        k = tf.minimum(self.k, tf.shape(power)[0])\n        idx = tf.math.top_k(power, k=k, sorted=True).indices\n        x = tf.gather(x, idx, axis=3)          # (B, Nr, Nt, K, 2)\n\n        # Per-path CNN\n        x = tf.transpose(x, [0, 3, 1, 2, 4])  # (B, K, Nr, Nt, 2)\n        b, k2, nr, nt, _ = (tf.shape(x)[i] for i in range(5))\n        x = tf.reshape(x, [-1, nr, nt, 3])     # (B*K, Nr, Nt, 3)  [real, imag, mag]\n        h = self.conv1(x)\n        h = self.conv2(h)\n        h = self.conv3(h)\n        h = self.gap(h)                         # (B*K, 64)\n        h = tf.reshape(h, [b, k2, -1])          # (B, K, 64)\n\n        # Attention pooling\n        att = tf.nn.softmax(self.att_dense(h), axis=1)\n        h = tf.reduce_sum(att * h, axis=1)      # (B, 64)\n        return self.proj(h, training=training)  # (B, emb_dim)\n",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "class BeamModel(keras.Model):\n    \"\"\"Beam-prediction-only federated model.\"\"\"\n\n    def __init__(self, nr: int, nt: int, top_k_paths: int,\n                 beam_codebook_size: int, dropout: float = 0.0):\n        super().__init__()\n        self.encoder = ChannelEncoder(nr, nt, top_k_paths, beam_codebook_size,\n                                      emb_dim=128, dropout=dropout)\n        self.shared1 = keras.layers.Dense(256, activation=\"relu\")\n        self.shared2 = keras.layers.Dense(128, activation=\"relu\")\n        self.beam_dense = keras.layers.Dense(64, activation=\"relu\")\n        self.beam_head = keras.layers.Dense(beam_codebook_size)  # logits\n\n    def call(self, x, training=False):\n        emb = self.encoder(x, training=training)\n        h = self.shared1(emb)\n        h = self.shared2(h)\n        h = self.beam_dense(h)\n        return self.beam_head(h)  # (B, beam_codebook_size)\n\n    def build_model(self, nr, nt, k):\n        dummy = tf.random.normal((1, nr, nt, k, 2))\n        _ = self(dummy, training=False)\n        self.built = True\n",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 8. Flower Client (Beam Only)"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "class BeamFlowerClient(fl.client.NumPyClient):\n    \"\"\"\n    Federated client \u2014 beam prediction only.\n    Uses focal loss with class weights and label smoothing.\n    \"\"\"\n\n    def __init__(self, model, dataset, train_indices, test_indices, cfg, trajectory_id):\n        self.model = model\n        self.dataset = dataset\n        self.train_indices = train_indices\n        self.test_indices = test_indices\n        self.cfg = cfg\n        self.trajectory_id = trajectory_id\n        self.optimizer = keras.optimizers.Adam(cfg[\"lr\"])\n        self.beam_class_weights = tf.constant(\n            np.load(\"beam_class_weights.npy\"), dtype=tf.float32\n        )\n        self.focal_gamma = cfg.get(\"focal_gamma\", 2.0)\n        self.label_smoothing = cfg.get(\"label_smoothing\", 0.05)\n        self.grad_clip_norm = cfg.get(\"grad_clip_norm\", 5.0)\n\n    # \u2500\u2500 Focal loss \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n    def focal_loss(self, labels, logits):\n        num_classes = tf.shape(logits)[-1]\n        if self.label_smoothing > 0:\n            oh = tf.one_hot(labels, depth=num_classes)\n            oh = oh * (1 - self.label_smoothing) + self.label_smoothing / tf.cast(num_classes, tf.float32)\n            ce = tf.nn.softmax_cross_entropy_with_logits(labels=oh, logits=logits)\n        else:\n            ce = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=labels, logits=logits)\n\n        probs = tf.nn.softmax(logits, axis=-1)\n        p_t = tf.reduce_sum(probs * tf.one_hot(labels, num_classes), axis=-1)\n        focal_w = tf.pow(1.0 - p_t, self.focal_gamma)\n        sample_w = tf.gather(self.beam_class_weights, labels)\n        return tf.reduce_mean(focal_w * sample_w * ce)\n\n    # \u2500\u2500 Data loading \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n    def _load(self, indices):\n        X, y = [], []\n        for i in indices:\n            x, labels = self.dataset[i]\n            X.append(x)\n            y.append(labels[\"beam_index\"])\n        return np.stack(X).astype(np.float32), np.array(y, dtype=np.int32)\n\n    def _ensure_built(self):\n        if self.model.built:\n            return\n        idx = self.train_indices[0] if self.train_indices else 0\n        x, _ = self.dataset[idx]\n        self.model(tf.convert_to_tensor(x[np.newaxis], dtype=tf.float32), training=False)\n        self.model.built = True\n\n    # \u2500\u2500 Flower interface \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n    def get_parameters(self, config):\n        self._ensure_built()\n        return self.model.get_weights()\n\n    def fit(self, parameters, config):\n        self._ensure_built()\n        self.model.set_weights(parameters)\n        X_train, y_train = self._load(self.train_indices)\n\n        for _ in range(self.cfg[\"local_epochs\"]):\n            with tf.GradientTape() as tape:\n                logits = self.model(\n                    tf.convert_to_tensor(X_train, tf.float32), training=True\n                )\n                loss = self.focal_loss(tf.convert_to_tensor(y_train, tf.int32), logits)\n\n            grads = tape.gradient(loss, self.model.trainable_weights)\n            safe_grads = [g if g is not None else tf.zeros_like(v)\n                          for g, v in zip(grads, self.model.trainable_weights)]\n            clipped, _ = tf.clip_by_global_norm(safe_grads, self.grad_clip_norm)\n            self.optimizer.apply_gradients(zip(clipped, self.model.trainable_weights))\n\n        return self.model.get_weights(), len(self.train_indices), {\n            \"loss\": float(loss.numpy()),\n            \"trajectory_id\": self.trajectory_id,\n        }\n\n    def evaluate(self, parameters, config):\n        self._ensure_built()\n        self.model.set_weights(parameters)\n        X_test, y_test = self._load(self.test_indices)\n\n        logits = self.model(tf.convert_to_tensor(X_test, tf.float32), training=False)\n        loss = self.focal_loss(tf.convert_to_tensor(y_test, tf.int32), logits)\n        preds = tf.argmax(logits, axis=1).numpy()\n        acc = float(np.mean(preds == y_test))\n        n_unique = int(len(np.unique(preds)))\n\n        return float(loss.numpy()), len(self.test_indices), {\n            \"beam_accuracy\"       : acc,\n            \"beam_num_unique_preds\": n_unique,\n            \"loss\"                : float(loss.numpy()),\n        }\n",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 9. Federated Training"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Model hyperparameters\nnr = 16\nnt = 16\ntop_k_paths = 6\nbeam_codebook_size = 16\n\n# Reset Ray\nif ray.is_initialized():\n    ray.shutdown()\n    print(\"Ray shutdown\")\n\n# Client-index mapping (deterministic)\n_client_id_to_idx: Dict = {}\n\ndef client_fn(context: fl.common.Context) -> fl.client.Client:\n    tf.random.set_seed(seed)\n    np.random.seed(seed)\n\n    node_id = context.node_id\n    if node_id not in _client_id_to_idx:\n        _client_id_to_idx[node_id] = len(_client_id_to_idx) % len(clients)\n    client_obj = clients[_client_id_to_idx[node_id]]\n\n    model = BeamModel(nr, nt, top_k_paths, beam_codebook_size)\n    model.build_model(nr, nt, top_k_paths)\n\n    return BeamFlowerClient(\n        model=model,\n        dataset=ds,\n        train_indices=client_obj.train_indices,\n        test_indices=client_obj.test_indices,\n        cfg=CFG,\n        trajectory_id=client_obj.trajectory_id,\n    ).to_client()\n\n\n# Global model for initial parameters\nglobal_model = BeamModel(nr, nt, top_k_paths, beam_codebook_size)\nglobal_model.build_model(nr, nt, top_k_paths)\n\n\ndef agg_metrics(metrics):\n    agg = {}\n    for _, m in metrics:\n        for k, v in m.items():\n            agg.setdefault(k, []).append(v)\n    return {k: float(np.mean(v)) for k, v in agg.items()\n            if isinstance(v[0], (int, float, np.integer, np.floating))}\n\n\nstrategy = fl.server.strategy.FedAvg(\n    fraction_fit              = CFG[\"client_frac\"],\n    fraction_evaluate         = CFG[\"client_frac\"],\n    min_fit_clients           = min(10, len(clients)),\n    min_evaluate_clients      = min(10, len(clients)),\n    min_available_clients     = len(clients),\n    initial_parameters        = fl.common.ndarrays_to_parameters(global_model.get_weights()),\n    fit_metrics_aggregation_fn    = agg_metrics,\n    evaluate_metrics_aggregation_fn = agg_metrics,\n)\n\nnum_rounds = 10\n_client_id_to_idx.clear()\nhistory = fl.simulation.start_simulation(\n    client_fn   = client_fn,\n    num_clients = len(clients),\n    config      = fl.server.ServerConfig(num_rounds=num_rounds),\n    strategy    = strategy,\n)\nprint(\"Training complete.\")\n",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 10. Results"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "import matplotlib.pyplot as plt\n\nfig, axes = plt.subplots(1, 3, figsize=(15, 4))\n\n# Loss curve\nif history.losses_distributed:\n    rounds, losses = zip(*history.losses_distributed)\n    axes[0].plot(rounds, losses, \"b-o\", linewidth=2, markersize=4)\naxes[0].set_xlabel(\"Round\"); axes[0].set_ylabel(\"Loss\")\naxes[0].set_title(\"Distributed Loss\"); axes[0].grid(True, alpha=0.3)\n\n# Beam accuracy\neval_m = history.metrics_distributed\nif eval_m and \"beam_accuracy\" in eval_m:\n    rds, accs = zip(*eval_m[\"beam_accuracy\"])\n    axes[1].plot(rds, [a * 100 for a in accs], \"g-o\", linewidth=2, markersize=4)\naxes[1].set_xlabel(\"Round\"); axes[1].set_ylabel(\"Accuracy (%)\")\naxes[1].set_title(\"Beam Accuracy\"); axes[1].grid(True, alpha=0.3)\n\n# Prediction diversity\nif eval_m and \"beam_num_unique_preds\" in eval_m:\n    rds, div = zip(*eval_m[\"beam_num_unique_preds\"])\n    axes[2].plot(rds, div, \"m-o\", linewidth=2, markersize=4)\n    axes[2].axhline(1, color=\"r\", linestyle=\"--\", alpha=0.5, label=\"Collapsed\")\n    axes[2].axhline(16, color=\"g\", linestyle=\"--\", alpha=0.5, label=\"Max diversity\")\n    axes[2].legend()\naxes[2].set_xlabel(\"Round\"); axes[2].set_ylabel(\"Unique Predictions\")\naxes[2].set_title(\"Beam Prediction Diversity\"); axes[2].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n",
   "outputs": [],
   "execution_count": null
  }
 ]
}